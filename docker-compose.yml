  services:  

    broker-1:
      build:
        context: .
        dockerfile: Dockerfile.kafka
      hostname: broker-1
      container_name: broker-1
      ports:
        - "9092:9092"
      environment:
        KAFKA_NODE_ID: 1
        CLUSTER_ID: 'MkU3OEVBNTcwNTJENDM2Qk'
        
        KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT'
        KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT://broker-1:29092,PLAINTEXT_HOST://localhost:9092'
        KAFKA_LISTENERS: 'PLAINTEXT://broker-1:29092,CONTROLLER://broker-1:29093,PLAINTEXT_HOST://0.0.0.0:9092'
        KAFKA_INTER_BROKER_LISTENER_NAME: 'PLAINTEXT'
        KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER'

        KAFKA_CONTROLLER_QUORUM_VOTERS: '1@broker-1:29093,2@broker-2:29093,3@broker-3:29093'
        KAFKA_PROCESS_ROLES: 'broker,controller'
      
        KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3 # This sets how many copies Kafka keeps of the consumer offset topic
        KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0 # How long Kafka waits before assigning partitions to consumers when a group starts.
        KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3

        KAFKA_LOG_DIRS: '/var/lib/kafka/data' 
        
        KAFKA_OPTS: -javaagent:/usr/app/jmx_prometheus_javaagent-0.20.0.jar=7071:/usr/app/kafka.yml
      volumes:
        - ./jmx_exporter/:/usr/app/
        - broker-1-data:/var/lib/kafka/data
      networks:
        - kafka-networks

    broker-2:
      build:
        context: .
        dockerfile: Dockerfile.kafka
      hostname: broker-2
      container_name: broker-2
      ports:
        - "9093:9092"
      environment:
        KAFKA_NODE_ID: 2
        CLUSTER_ID: 'MkU3OEVBNTcwNTJENDM2Qk'
        
        KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT'

        KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT://broker-2:29092,PLAINTEXT_HOST://localhost:9093'
        KAFKA_LISTENERS: 'PLAINTEXT://broker-2:29092,CONTROLLER://broker-2:29093,PLAINTEXT_HOST://0.0.0.0:9092'
        KAFKA_INTER_BROKER_LISTENER_NAME: 'PLAINTEXT'
        KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER'

        KAFKA_CONTROLLER_QUORUM_VOTERS: '1@broker-1:29093,2@broker-2:29093,3@broker-3:29093'
        KAFKA_PROCESS_ROLES: 'broker,controller'
      

        

        KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
        KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
        KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3

        
        KAFKA_OPTS: -javaagent:/usr/app/jmx_prometheus_javaagent-0.20.0.jar=7071:/usr/app/kafka.yml
        KAFKA_LOG_DIRS: '/var/lib/kafka/data'
      volumes:
        - ./jmx_exporter/:/usr/app/
        - broker-2-data:/var/lib/kafka/data
      networks:
        - kafka-networks

    broker-3:
      build:
        context: .
        dockerfile: Dockerfile.kafka
      hostname: broker-3
      container_name: broker-3
      ports:
        - "9094:9092"
      environment:
        KAFKA_NODE_ID: 3
        CLUSTER_ID: 'MkU3OEVBNTcwNTJENDM2Qk'

        KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT'

        KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT://broker-3:29092,PLAINTEXT_HOST://localhost:9094'
        KAFKA_LISTENERS: 'PLAINTEXT://broker-3:29092,CONTROLLER://broker-3:29093,PLAINTEXT_HOST://0.0.0.0:9092'
        KAFKA_INTER_BROKER_LISTENER_NAME: 'PLAINTEXT'
        KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER'

        KAFKA_CONTROLLER_QUORUM_VOTERS: '1@broker-1:29093,2@broker-2:29093,3@broker-3:29093'
        KAFKA_PROCESS_ROLES: 'broker,controller'
      

        

        KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
        KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
        KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3

        KAFKA_LOG_DIRS: '/var/lib/kafka/data'
        KAFKA_OPTS: -javaagent:/usr/app/jmx_prometheus_javaagent-0.20.0.jar=7071:/usr/app/kafka.yml
      volumes:
        - ./jmx_exporter/:/usr/app/
        - broker-3-data:/var/lib/kafka/data
      networks:
        - kafka-networks

      

    schema-registry:
      image: confluentinc/cp-schema-registry:8.0.0
      hostname: schema-registry
      container_name: schema-registry
      depends_on:
        - broker-1
        - broker-2
        - broker-3
      ports:
        - "8081:8081"
      environment:
        SCHEMA_REGISTRY_HOST_NAME: schema-registry
        SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081
        SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: 'broker-1:29092,broker-2:29092,broker-3:29092'
      networks:
        - kafka-networks

    connect:
      image: cnfldemos/kafka-connect-datagen:0.6.4-7.6.0
      hostname: connect
      container_name: connect
      depends_on:
        - broker-1
        - broker-2
        - broker-3
        - schema-registry
      ports:
        - "8083:8083"
      restart: on-failure:10 
      environment:
        CONNECT_BOOTSTRAP_SERVERS: 'broker-1:29092,broker-2:29092,broker-3:29092'
        CONNECT_REST_ADVERTISED_HOST_NAME: connect
        CONNECT_GROUP_ID: compose-connect-group
        CONNECT_CONFIG_STORAGE_TOPIC: docker-connect-configs
        CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 3
        CONNECT_OFFSET_FLUSH_INTERVAL_MS: 10000
        CONNECT_OFFSET_STORAGE_TOPIC: docker-connect-offsets
        CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 3
        CONNECT_STATUS_STORAGE_TOPIC: docker-connect-status
        CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 3
        CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter
        CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
        CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8081
        CONNECT_PLUGIN_PATH: "/usr/share/java,/usr/share/confluent-hub-components"

    ksqldb-server:
      image: confluentinc/cp-ksqldb-server:8.0.0
      hostname: ksqldb-server
      container_name: ksqldb-server
      restart: on-failure
      depends_on:
        - broker-1
        - broker-2
        - broker-3
        - connect
      ports:
        - "8088:8088"
      environment:
        KSQL_CONFIG_DIR: "/etc/ksql"
        KSQL_BOOTSTRAP_SERVERS: "broker-1:29092,broker-2:29092,broker-3:29092"
        KSQL_HOST_NAME: ksqldb-server
        KSQL_LISTENERS: "http://0.0.0.0:8088"
        KSQL_CACHE_MAX_BYTES_BUFFERING: 0
        KSQL_KSQL_SCHEMA_REGISTRY_URL: "http://schema-registry:8081"
        KSQL_KSQL_CONNECT_URL: "http://connect:8083"
        KSQL_KSQL_LOGGING_PROCESSING_TOPIC_REPLICATION_FACTOR: 1
        KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: 'true'
        KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: 'true'

    ksqldb-cli:
      image: confluentinc/cp-ksqldb-cli:8.0.0
      container_name: ksqldb-cli
      depends_on:
        - broker-1
        - broker-2
        - broker-3
        - connect
        - ksqldb-server
      entrypoint: /bin/sh
      tty: true
      networks:
        - kafka-networks

    ksql-datagen:
      image: confluentinc/ksqldb-examples:8.0.0
      hostname: ksql-datagen
      container_name: ksql-datagen
      depends_on:
        - ksqldb-server
        - broker-1
        - broker-2
        - broker-3
        - schema-registry
        - connect
      command: "bash -c 'echo Waiting for Kafka to be ready... && \
                        cub kafka-ready -b broker-1:29092 1 40 && \
                        echo Waiting for Confluent Schema Registry to be ready... && \
                        cub sr-ready schema-registry 8081 40 && \
                        echo Waiting a few seconds for topic creation to finish... && \
                        sleep 11 && \
                        tail -f /dev/null'"
      environment:
        KSQL_CONFIG_DIR: "/etc/ksql"
        STREAMS_BOOTSTRAP_SERVERS: broker-1:29092,broker-2:29092,broker-3:29092
        STREAMS_SCHEMA_REGISTRY_HOST: schema-registry
        STREAMS_SCHEMA_REGISTRY_PORT: 8081
      networks:
        - kafka-networks

    rest-proxy:
      image: confluentinc/cp-kafka-rest:8.0.0
      depends_on:
        - broker-1
        - broker-2
        - broker-3
        - schema-registry
      ports:
        - 8082:8082
      hostname: rest-proxy
      container_name: rest-proxy
      environment:
        KAFKA_REST_HOST_NAME: rest-proxy
        KAFKA_REST_BOOTSTRAP_SERVERS: 'broker-1:29092,broker-2:29092,broker-3:29092'
        KAFKA_REST_LISTENERS: "http://0.0.0.0:8082"
        KAFKA_REST_SCHEMA_REGISTRY_URL: 'http://schema-registry:8081'
      networks:
        - kafka-networks


    postgres:
      image: postgres:14
      environment:
        POSTGRES_USER: prefect
        POSTGRES_PASSWORD: prefect
        POSTGRES_DB: prefect
      volumes:
        - postgres_data:/var/lib/postgresql/data
      healthcheck:
        test: ["CMD-SHELL", "pg_isready -U prefect"]
        interval: 5s
        timeout: 5s
        retries: 5
      networks:
        - kafka-networks

    redis:
      image: redis:7
      volumes:
        - redis_data:/data
      healthcheck:
        test: ["CMD-SHELL", "redis-cli ping"]
        interval: 5s
        timeout: 5s
        retries: 5
      networks:
        - kafka-networks

    prefect-server:
      image: prefecthq/prefect:3-latest
      depends_on:
        postgres:
          condition: service_healthy
        redis:
          condition: service_healthy
      healthcheck:
        test: ["CMD", "curl", "-f", "http://localhost:4200/api/health"]
        interval: 10s
        timeout: 10s
        retries: 5
        start_period: 10s
        
      environment:
        PREFECT_API_DATABASE_CONNECTION_URL: postgresql+asyncpg://prefect:prefect@postgres:5432/prefect
        PREFECT_SERVER_API_HOST: 0.0.0.0
        PREFECT_MESSAGING_BROKER: prefect_redis.messaging
        PREFECT_MESSAGING_CACHE: prefect_redis.messaging
        PREFECT_REDIS_MESSAGING_HOST: redis
        PREFECT_REDIS_MESSAGING_PORT: 6379
        PREFECT_REDIS_MESSAGING_DB: 0
      command: prefect server start --no-services
      ports:
        - "4200:4200"
      networks:
        - kafka-networks

    prefect-services:
      image: prefecthq/prefect:3-latest
      depends_on:
        postgres:
          condition: service_healthy
        redis:
          condition: service_healthy
      environment:
        PREFECT_API_DATABASE_CONNECTION_URL: postgresql+asyncpg://prefect:prefect@postgres:5432/prefect
        PREFECT_MESSAGING_BROKER: prefect_redis.messaging
        PREFECT_MESSAGING_CACHE: prefect_redis.messaging
        PREFECT_REDIS_MESSAGING_HOST: redis
        PREFECT_REDIS_MESSAGING_PORT: 6379
        PREFECT_REDIS_MESSAGING_DB: 0
      command: prefect server services start
      networks:
        - kafka-networks

    prefect-worker:
      image: prefecthq/prefect:3-latest
      depends_on:
        prefect-server:
          condition: service_healthy
      environment:
        PREFECT_API_URL: http://prefect-server:4200/api
      command:  "prefect worker start --pool local-pool"
      networks:
        - kafka-networks

    elasticsearch:
      image: docker.elastic.co/elasticsearch/elasticsearch:8.13.4
      container_name: elasticsearch
      environment:
        - discovery.type=single-node
        - bootstrap.memory_lock=true
        - ES_JAVA_OPTS=-Xms1g -Xmx1g
        - xpack.security.enabled=false
        - xpack.security.enrollment.enabled=false
        - xpack.security.http.ssl.enabled=false
        - xpack.security.transport.ssl.enabled=false
      ulimits:
        memlock:
          soft: -1
          hard: -1
      volumes:
        - es_data:/usr/share/elasticsearch/data
      ports:
        - "9200:9200"
      networks:
        - kafka-networks

    kibana:
      image: docker.elastic.co/kibana/kibana:8.13.4
      container_name: kibana
      environment:
        - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
      ports:
        - "5601:5601"
      depends_on:
        - elasticsearch
      networks:
        - kafka-networks


    spark-master:
      build:
        context: .
        dockerfile: Dockerfile.spark
      container_name: spark-master
      hostname: spark-master
      ports:
        - "8080:8080"
        - "7077:7077"
      environment:
        - SPARK_MODE=master
        - SPARK_MASTER_HOST=spark-master
        - SPARK_MASTER_PORT=7077
        - SPARK_MASTER_WEBUI_PORT=8080
        - SPARK_MASTER_OPTS=-Dspark.metrics.conf.master.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink -javaagent:/opt/spark/jars/jmx_prometheus_javaagent-0.20.0.jar=7072:/opt/spark/conf/spark.yml
      volumes:
        - ./src:/app/src
        - spark_data:/opt/spark-data
        - ./app/spark:/opt/spark-apps
        - ./jmx_exporter/jmx_prometheus_javaagent-0.20.0.jar:/opt/spark/jars/jmx_prometheus_javaagent-0.20.0.jar
        - ./jmx_exporter/spark.yml:/opt/spark/conf/spark.yml
      
      networks:
        - kafka-networks
      command: bash -c "/opt/spark/sbin/start-master.sh && tail -f /dev/null"
    
    spark-worker-1:
      build:
        context: .
        dockerfile: Dockerfile.spark
      container_name: spark-worker-1
      hostname: spark-worker-1
      depends_on:
        - spark-master
      ports:
        - "8084:8081"  
      environment:
        - SPARK_MODE=worker
        - SPARK_WORKER_OPTS=-Dspark.metrics.conf.worker.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink -javaagent:/opt/spark/jars/jmx_prometheus_javaagent-0.20.0.jar=7073:/opt/spark/conf/spark.yml
       
        - SPARK_MASTER=spark://spark-master:7077
        - SPARK_WORKER_CORES=2
        - SPARK_WORKER_MEMORY=8g
        - SPARK_WORKER_WEBUI_PORT=8081
      volumes:
        - ./src:/app/src
        - spark_data:/opt/spark-data
        - ./jmx_exporter/jmx_prometheus_javaagent-0.20.0.jar:/opt/spark/jars/jmx_prometheus_javaagent-0.20.0.jar
        - ./jmx_exporter/spark.yml:/opt/spark/conf/spark.yml
      networks:
        - kafka-networks
      command: bash -c "/opt/spark/sbin/start-worker.sh spark://spark-master:7077 && tail -f /dev/null"


    spark-streaming-submissions:
      build:
        context: .
        dockerfile: Dockerfile.spark
      container_name: spark-streaming-submissions
      depends_on:
        - spark-master
        - broker-1
        - broker-2
        - broker-3
      environment:
        - SPARK_MASTER=spark://spark-master:7077
        - KAFKA_BOOTSTRAP_SERVERS=broker-1:29092,broker-2:29092,broker-3:29092
        - SPARK_SAVE_DIR=/opt/spark-data
        - JAVA_TOOL_OPTIONS=-Dspark.metrics.conf.driver.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink -javaagent:/opt/spark/jars/jmx_prometheus_javaagent-0.20.0.jar=7074:/opt/spark/conf/spark.yml
      
      volumes:
        - ./src:/app/src
        - spark_data:/opt/spark-data
        - ./jmx_exporter/jmx_prometheus_javaagent-0.20.0.jar:/opt/spark/jars/jmx_prometheus_javaagent-0.20.0.jar
        - ./jmx_exporter/spark.yml:/opt/spark/conf/spark.yml
      networks:
        - kafka-networks
      # command: |
      #   bash -c "uv run src/spark/streaming_es.py   --topic reddit.submissions   --bootstrap broker-1:29092,broker-2:29092,broker-3:29092  --save-dir /opt/spark-data   --es-index reddit_submissions  --es-id-field id"
      # command: bash -c "tail -f /dev/null"
      restart: unless-stopped

    spark-streaming-comments:
      build:
        context: .
        dockerfile: Dockerfile.spark
      container_name: spark-streaming-comments
      depends_on:
        - spark-master
        - broker-1
        - broker-2
        - broker-3
      environment:
        - SPARK_MASTER=spark://spark-master:7077
        - KAFKA_BOOTSTRAP_SERVERS=broker-1:29092,broker-2:29092,broker-3:29092
        - SPARK_SAVE_DIR=/opt/spark-data
        - JAVA_TOOL_OPTIONS=-Dspark.metrics.conf.driver.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink -javaagent:/opt/spark/jars/jmx_prometheus_javaagent-0.20.0.jar=7075:/opt/spark/conf/spark.yml
    
      volumes:
        - ./src:/app/src
        - spark_data:/opt/spark-data
        - ./jmx_exporter/jmx_prometheus_javaagent-0.20.0.jar:/opt/spark/jars/jmx_prometheus_javaagent-0.20.0.jar
        - ./jmx_exporter/spark.yml:/opt/spark/conf/spark.yml
      networks:
        - kafka-networks
      # command: |
      #   bash -c "uv run src/spark/streaming_es.py   --topic reddit.comments   --bootstrap broker-1:29092,broker-2:29092,broker-3:29092  --save-dir /opt/spark-data   --es-index reddit_comments  --es-id-field id"
      # command: bash -c "tail -f /dev/null"
      restart: unless-stopped

    prefect-custom-worker:
      build:
        context: .
        dockerfile: Dockerfile.prefect-worker
      container_name: prefect-custom-worker 
      depends_on:
        prefect-server:
          condition: service_started
        broker-1:
          condition: service_started
      environment:
        PREFECT_API_URL: http://prefect-server:4200/api
        KAFKA_BOOTSTRAP_SERVERS: broker-1:29092,broker-2:29092,broker-3:29092
        KAFKA_CLIENT_ID: reddit-producer
        REDDIT_CLIENT_ID: ${REDDIT_CLIENT_ID}
        REDDIT_CLIENT_SECRET: ${REDDIT_CLIENT_SECRET}
        REDDIT_USER_AGENT: ${REDDIT_USER_AGENT}
        REDDIT_SUBREDDIT_LIMIT: 10000
        REDDIT_POSTS_PER_SUBREDDIT: 10000
        REDDIT_COMMENT_LIMIT: 10000
        REDDIT_POST_SORT: hot
        TOPIC_REDDIT_SUBMISSIONS: reddit.submissions
        TOPIC_REDDIT_COMMENTS: reddit.comments
      volumes:
        - ./src:/app/src
      networks:
        - kafka-networks
      # command: bash -c "sleep 30 && uv run prefect worker start --pool default & uv run src/flow.py"
    
    node-exporter:
      image: prom/node-exporter:v1.3.1
      container_name: node-exporterpeo
      volumes:
        - /proc:/host/proc:ro
        - /sys:/host/sys:ro
        - /:/rootfs:ro
      command:
        - '--path.procfs=/host/proc'
        - '--path.rootfs=/rootfs'
        - '--path.sysfs=/host/sys'
        - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
      ports:
        - 9100:9100
      networks:
        - monitoring

    prometheus:
      image: prom/prometheus:v3.0.0
      container_name: prometheus
      volumes:
        - ./monitoring/prometheus:/etc/prometheus
      depends_on:
        - broker-1
        - broker-2
        - broker-3
      ports:
        - 9090:9090
      networks:
        - monitoring
        - kafka-networks
      

    grafana:
      image: grafana/grafana:9.0.5
      container_name: grafana
      volumes:
        - grafana_data:/var/lib/grafana
        - ./monitoring/graphana/provisioning:/etc/grafana/provisioning
        - ./monitoring/graphana/dashboards:/var/lib/grafana/dashboards
      ports:
        - 3000:3000
      networks:
        - monitoring
      healthcheck:
        test: ["CMD-SHELL", "curl -f localhost:3000/api/health && echo 'ready'"]
        interval: 10s
        retries: 10        

  volumes:
    broker-1-data:
    broker-2-data:
    broker-3-data:
    postgres_data:
      driver: local
      driver_opts:
        type: none
        o: bind
        device: ./infra/postgres_data
    redis_data:
      driver: local
      driver_opts:
        type: none
        o: bind
        device: ./infra/redis_data
    es_data:
      driver: local
    spark_data:
      driver: local
    grafana_data:
      driver: local
    prometheus_data:
      driver: local


  networks:
    kafka-networks:
      driver: bridge
    monitoring:
      driver: bridge
    